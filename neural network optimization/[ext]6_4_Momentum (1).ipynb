{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el8l05WQEO46"
   },
   "source": [
    "# **Notebook 6.4: Momentum**\n",
    "\n",
    "This notebook investigates the use of momentum as illustrated in figure 6.7 from the book.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhmIOLiZELV_"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cRkrh9MZ58Z"
   },
   "outputs": [],
   "source": [
    "# Let's create our training data of 30 pairs {x_i, y_i}\n",
    "# We'll try to fit the Gabor model to these data\n",
    "data = np.array([[-1.920e+00,-1.422e+01,1.490e+00,-1.940e+00,-2.389e+00,-5.090e+00,\n",
    "                 -8.861e+00,3.578e+00,-6.010e+00,-6.995e+00,3.634e+00,8.743e-01,\n",
    "                 -1.096e+01,4.073e-01,-9.467e+00,8.560e+00,1.062e+01,-1.729e-01,\n",
    "                  1.040e+01,-1.261e+01,1.574e-01,-1.304e+01,-2.156e+00,-1.210e+01,\n",
    "                 -1.119e+01,2.902e+00,-8.220e+00,-1.179e+01,-8.391e+00,-4.505e+00],\n",
    "                  [-1.051e+00,-2.482e-02,8.896e-01,-4.943e-01,-9.371e-01,4.306e-01,\n",
    "                  9.577e-03,-7.944e-02 ,1.624e-01,-2.682e-01,-3.129e-01,8.303e-01,\n",
    "                  -2.365e-02,5.098e-01,-2.777e-01,3.367e-01,1.927e-01,-2.222e-01,\n",
    "                  6.352e-02,6.888e-03,3.224e-02,1.091e-02,-5.706e-01,-5.258e-02,\n",
    "                  -3.666e-02,1.709e-01,-4.805e-02,2.008e-01,-1.904e-01,5.952e-01]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQUERmb2erAe"
   },
   "outputs": [],
   "source": [
    "# Let's define our model\n",
    "def model(phi,x):\n",
    "  sin_component = np.sin(phi[0] + 0.06 * phi[1] * x)\n",
    "  gauss_component = np.exp(-(phi[0] + 0.06 * phi[1] * x) * (phi[0] + 0.06 * phi[1] * x) / 32)\n",
    "  y_pred= sin_component * gauss_component\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFRe9POHF2le"
   },
   "outputs": [],
   "source": [
    "# Draw model\n",
    "def draw_model(data,model,phi,title=None):\n",
    "  x_model = np.arange(-15,15,0.1)\n",
    "  y_model = model(phi,x_model)\n",
    "\n",
    "  fix, ax = plt.subplots()\n",
    "  ax.plot(data[0,:],data[1,:],'bo')\n",
    "  ax.plot(x_model,y_model,'m-')\n",
    "  ax.set_xlim([-15,15]);ax.set_ylim([-1,1])\n",
    "  ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "  if title is not None:\n",
    "    ax.set_title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXx1Tpd1Tl-I"
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters and draw the model\n",
    "phi = np.zeros((2,1))\n",
    "phi[0] =  -5     # Horizontal offset\n",
    "phi[1] =  25     # Frequency\n",
    "draw_model(data,model,phi, \"Initial parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU5mdGvpTtEG"
   },
   "source": [
    "Now let's compute the sum of squares loss for the training data and plot the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7dqTY2Gg7CR"
   },
   "outputs": [],
   "source": [
    "def compute_loss(data_x, data_y, model, phi):\n",
    "  pred_y = model(phi, data_x)\n",
    "  loss = np.sum((pred_y-data_y)*(pred_y-data_y))\n",
    "  return loss\n",
    "\n",
    "def draw_loss_function(compute_loss, data,  model, phi_iters = None):\n",
    "  # Define pretty colormap\n",
    "  my_colormap_vals_hex =('2a0902', '2b0a03', '2c0b04', '2d0c05', '2e0c06', '2f0d07', '300d08', '310e09', '320f0a', '330f0b', '34100b', '35110c', '36110d', '37120e', '38120f', '39130f', '3a1410', '3b1411', '3c1511', '3d1612', '3e1613', '3f1713', '401714', '411814', '421915', '431915', '451a16', '461b16', '471b17', '481c17', '491d18', '4a1d18', '4b1e19', '4c1f19', '4d1f1a', '4e201b', '50211b', '51211c', '52221c', '53231d', '54231d', '55241e', '56251e', '57261f', '58261f', '592720', '5b2821', '5c2821', '5d2922', '5e2a22', '5f2b23', '602b23', '612c24', '622d25', '632e25', '652e26', '662f26', '673027', '683027', '693128', '6a3229', '6b3329', '6c342a', '6d342a', '6f352b', '70362c', '71372c', '72372d', '73382e', '74392e', '753a2f', '763a2f', '773b30', '783c31', '7a3d31', '7b3e32', '7c3e33', '7d3f33', '7e4034', '7f4134', '804235', '814236', '824336', '834437', '854538', '864638', '874739', '88473a', '89483a', '8a493b', '8b4a3c', '8c4b3c', '8d4c3d', '8e4c3e', '8f4d3f', '904e3f', '924f40', '935041', '945141', '955242', '965343', '975343', '985444', '995545', '9a5646', '9b5746', '9c5847', '9d5948', '9e5a49', '9f5a49', 'a05b4a', 'a15c4b', 'a35d4b', 'a45e4c', 'a55f4d', 'a6604e', 'a7614e', 'a8624f', 'a96350', 'aa6451', 'ab6552', 'ac6552', 'ad6653', 'ae6754', 'af6855', 'b06955', 'b16a56', 'b26b57', 'b36c58', 'b46d59', 'b56e59', 'b66f5a', 'b7705b', 'b8715c', 'b9725d', 'ba735d', 'bb745e', 'bc755f', 'bd7660', 'be7761', 'bf7862', 'c07962', 'c17a63', 'c27b64', 'c27c65', 'c37d66', 'c47e67', 'c57f68', 'c68068', 'c78169', 'c8826a', 'c9836b', 'ca846c', 'cb856d', 'cc866e', 'cd876f', 'ce886f', 'ce8970', 'cf8a71', 'd08b72', 'd18c73', 'd28d74', 'd38e75', 'd48f76', 'd59077', 'd59178', 'd69279', 'd7937a', 'd8957b', 'd9967b', 'da977c', 'da987d', 'db997e', 'dc9a7f', 'dd9b80', 'de9c81', 'de9d82', 'df9e83', 'e09f84', 'e1a185', 'e2a286', 'e2a387', 'e3a488', 'e4a589', 'e5a68a', 'e5a78b', 'e6a88c', 'e7aa8d', 'e7ab8e', 'e8ac8f', 'e9ad90', 'eaae91', 'eaaf92', 'ebb093', 'ecb295', 'ecb396', 'edb497', 'eeb598', 'eeb699', 'efb79a', 'efb99b', 'f0ba9c', 'f1bb9d', 'f1bc9e', 'f2bd9f', 'f2bfa1', 'f3c0a2', 'f3c1a3', 'f4c2a4', 'f5c3a5', 'f5c5a6', 'f6c6a7', 'f6c7a8', 'f7c8aa', 'f7c9ab', 'f8cbac', 'f8ccad', 'f8cdae', 'f9ceb0', 'f9d0b1', 'fad1b2', 'fad2b3', 'fbd3b4', 'fbd5b6', 'fbd6b7', 'fcd7b8', 'fcd8b9', 'fcdaba', 'fddbbc', 'fddcbd', 'fddebe', 'fddfbf', 'fee0c1', 'fee1c2', 'fee3c3', 'fee4c5', 'ffe5c6', 'ffe7c7', 'ffe8c9', 'ffe9ca', 'ffebcb', 'ffeccd', 'ffedce', 'ffefcf', 'fff0d1', 'fff2d2', 'fff3d3', 'fff4d5', 'fff6d6', 'fff7d8', 'fff8d9', 'fffada', 'fffbdc', 'fffcdd', 'fffedf', 'ffffe0')\n",
    "  my_colormap_vals_dec = np.array([int(element,base=16) for element in my_colormap_vals_hex])\n",
    "  r = np.floor(my_colormap_vals_dec/(256*256))\n",
    "  g = np.floor((my_colormap_vals_dec - r *256 *256)/256)\n",
    "  b = np.floor(my_colormap_vals_dec - r * 256 *256 - g * 256)\n",
    "  my_colormap = ListedColormap(np.vstack((r,g,b)).transpose()/255.0)\n",
    "\n",
    "  # Make grid of offset/frequency values to plot\n",
    "  offsets_mesh, freqs_mesh = np.meshgrid(np.arange(-10,10.0,0.1), np.arange(2.5,22.5,0.1))\n",
    "  loss_mesh = np.zeros_like(freqs_mesh)\n",
    "  # Compute loss for every set of parameters\n",
    "  for idslope, slope in np.ndenumerate(freqs_mesh):\n",
    "     loss_mesh[idslope] = compute_loss(data[0,:], data[1,:], model, np.array([[offsets_mesh[idslope]], [slope]]))\n",
    "\n",
    "  fig,ax = plt.subplots()\n",
    "  fig.set_size_inches(8,8)\n",
    "  ax.contourf(offsets_mesh,freqs_mesh,loss_mesh,256,cmap=my_colormap)\n",
    "  ax.contour(offsets_mesh,freqs_mesh,loss_mesh,20,colors=['#80808080'])\n",
    "  if phi_iters is not None:\n",
    "    ax.plot(phi_iters[0,:], phi_iters[1,:],'go-')\n",
    "  ax.set_ylim([2.5,22.5])\n",
    "  ax.set_xlabel('Offset $\\phi_{0}$'); ax.set_ylabel('Frequency, $\\phi_{1}$')\n",
    "  plt.show()\n",
    "\n",
    "draw_loss_function(compute_loss, data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Duf05WqqSC"
   },
   "source": [
    "As before, we compute the gradient vector for a given set of parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol\\phi} = \\begin{bmatrix}\\frac{\\partial L}{\\partial \\phi_0} \\\\\\frac{\\partial L}{\\partial \\phi_1} \\end{bmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpswmkL2qwBT"
   },
   "outputs": [],
   "source": [
    "# These came from writing out the expression for the sum of squares loss and taking the\n",
    "# derivative with respect to phi0 and phi1. It was a lot of hassle to get it right!\n",
    "def gabor_deriv_phi0(data_x,data_y,phi0, phi1):\n",
    "    x = 0.06 * phi1 * data_x + phi0\n",
    "    y = data_y\n",
    "    cos_component = np.cos(x)\n",
    "    sin_component = np.sin(x)\n",
    "    gauss_component = np.exp(-0.5 * x *x / 16)\n",
    "    deriv = cos_component * gauss_component - sin_component * gauss_component * x / 16\n",
    "    deriv = 2* deriv * (sin_component * gauss_component - y)\n",
    "    return np.sum(deriv)\n",
    "\n",
    "def gabor_deriv_phi1(data_x, data_y,phi0, phi1):\n",
    "    x = 0.06 * phi1 * data_x + phi0\n",
    "    y = data_y\n",
    "    cos_component = np.cos(x)\n",
    "    sin_component = np.sin(x)\n",
    "    gauss_component = np.exp(-0.5 * x *x / 16)\n",
    "    deriv = 0.06 * data_x * cos_component * gauss_component - 0.06 * data_x*sin_component * gauss_component * x / 16\n",
    "    deriv = 2*deriv * (sin_component * gauss_component - y)\n",
    "    return np.sum(deriv)\n",
    "\n",
    "def compute_gradient(data_x, data_y, phi):\n",
    "    dl_dphi0 = gabor_deriv_phi0(data_x, data_y, phi[0],phi[1])\n",
    "    dl_dphi1 = gabor_deriv_phi1(data_x, data_y, phi[0],phi[1])\n",
    "    # Return the gradient\n",
    "    return np.array([[dl_dphi0],[dl_dphi1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Tv3d4zqAdZR"
   },
   "source": [
    "Let's first run standard stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "469OP_UHskJ4"
   },
   "outputs": [],
   "source": [
    "# Set the random number generator so you always get same numbers (disable if you don't want this)\n",
    "np.random.seed(1)\n",
    "# Initialize the parameters\n",
    "n_steps = 81\n",
    "batch_size = 5\n",
    "alpha = 0.6\n",
    "phi_all = np.zeros((2,n_steps+1))\n",
    "phi_all[0,0] = -1.5\n",
    "phi_all[1,0] = 6.5\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
    "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
    "\n",
    "for c_step in range (n_steps):\n",
    "  # Choose random batch indices\n",
    "  batch_index = np.random.permutation(data.shape[1])[0:batch_size]\n",
    "  # Compute the gradient\n",
    "  gradient = compute_gradient(data[0,batch_index], data[1,batch_index], phi_all[:,c_step:c_step+1] )\n",
    "  # Update the parameters\n",
    "  phi_all[:,c_step+1:c_step+2] = phi_all[:,c_step:c_step+1] - alpha * gradient\n",
    "\n",
    "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
    "draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
    "draw_loss_function(compute_loss, data, model,phi_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMILovgMFpdI"
   },
   "source": [
    "Now let's add momentum (equation 6.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWBU8ZbSFny9"
   },
   "outputs": [],
   "source": [
    "# Set the random number generator so you always get same numbers (disable if you don't want this)\n",
    "np.random.seed(1)\n",
    "# Initialize the parameters\n",
    "n_steps = 81\n",
    "batch_size = 5\n",
    "alpha = 0.6\n",
    "beta = 0.6\n",
    "momentum = np.zeros([2,1])\n",
    "phi_all = np.zeros((2,n_steps+1))\n",
    "phi_all[0,0] = -1.5\n",
    "phi_all[1,0] = 6.5\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
    "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
    "\n",
    "for c_step in range (n_steps):\n",
    "  # Choose random batch indices\n",
    "  batch_index = np.random.permutation(data.shape[1])[0:batch_size]\n",
    "  # Compute the gradient\n",
    "  gradient = compute_gradient(data[0,batch_index], data[1,batch_index],  phi_all[:,c_step:c_step+1])\n",
    "  # TODO -- calculate momentum - replace the line below\n",
    "  momentum = beta * momentum + gradient\n",
    "\n",
    "  # Update the parameters\n",
    "  phi_all[:,c_step+1:c_step+2] = phi_all[:,c_step:c_step+1] - alpha * momentum\n",
    "\n",
    "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
    "draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
    "draw_loss_function(compute_loss, data, model,phi_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYIAomA-KPkU"
   },
   "source": [
    "Finally, we'll try Nesterov momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtwWeCZ5HLLh"
   },
   "outputs": [],
   "source": [
    "# Set the random number generator so you always get same numbers (disable if you don't want this)\n",
    "np.random.seed(1)\n",
    "# Initialize the parameters\n",
    "n_steps = 81\n",
    "batch_size = 5\n",
    "alpha = 0.6\n",
    "beta = 0.6\n",
    "momentum = np.zeros([2,1])\n",
    "phi_all = np.zeros((2,n_steps+1))\n",
    "phi_all[0,0] = -1.5\n",
    "phi_all[1,0] = 6.5\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
    "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
    "\n",
    "for c_step in range (n_steps):\n",
    "  # Choose random batch indices\n",
    "  batch_index = np.random.permutation(data.shape[1])[0:batch_size]\n",
    "  # TODO -- calculate Nesterov momentum - replace the lines below\n",
    "  # Compute the gradient at the lookahead point\n",
    "  phi_lookahead = phi_all[:,c_step:c_step+1] - alpha * beta * momentum\n",
    "  gradient = compute_gradient(data[0,batch_index], data[1,batch_index], phi_lookahead)\n",
    "  # Update the momentum\n",
    "  momentum = beta * momentum + gradient\n",
    "\n",
    "  # Update the parameters\n",
    "  phi_all[:,c_step+1:c_step+2] = phi_all[:,c_step:c_step+1] - alpha * momentum\n",
    "\n",
    "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
    "draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
    "draw_loss_function(compute_loss, data, model,phi_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-As4hS8s2nm"
   },
   "source": [
    "Note that for this case, Nesterov momentum does not improve the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d4f24",
   "metadata": {},
   "source": [
    "## Impact of L2 Regularization\n",
    "\n",
    "L2 regularization adds a penalty term proportional to the square of the magnitude of the weights to the loss function:\n",
    "\n",
    "$$L_{reg}(\\phi) = L(\\phi) + \\frac{\\lambda}{2} \\sum_{j} \\phi_j^2$$\n",
    "\n",
    "The gradient then becomes:\n",
    "\n",
    "$$\\frac{\\partial L_{reg}}{\\partial \\phi} = \\frac{\\partial L}{\\partial \\phi} + \\lambda \\phi$$\n",
    "\n",
    "This has the effect of pulling the parameters towards the origin, which can help prevent overfitting. Let's see how it affects our three optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_l2(data_x, data_y, model, phi, lam):\n",
    "  return compute_loss(data_x, data_y, model, phi) + 0.5 * lam * np.sum(phi**2)\n",
    "\n",
    "def compute_gradient_l2(data_x, data_y, phi, lam):\n",
    "  return compute_gradient(data_x, data_y, phi) + lam * phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599538c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-Side Comparison: No Regularization vs. L2 Regularization (Interactive)\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def run_and_plot(lmbda):\n",
    "    x = data[0,:]\n",
    "    y = data[1,:]\n",
    "\n",
    "    # Set the random number generator\n",
    "    np.random.seed(1)\n",
    "    n_steps = 81\n",
    "    batch_size = 5\n",
    "    alpha = 0.6\n",
    "    beta = 0.6\n",
    "    lambdas = [0.0, lmbda] \n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "\n",
    "    # Tighter axes limits to \"zoom in\" on the actual action\n",
    "    x_min, x_max = -7, 7\n",
    "    y_min, y_max = -5, 20\n",
    "\n",
    "    for idx, lmb in enumerate(lambdas):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Run SGD, Momentum, and Nesterov for this lambda\n",
    "        alg_results = []\n",
    "        labels = ['SGD', 'Momentum', 'Nesterov']\n",
    "        colors = ['#FF4B4B', '#4B8BFF', '#2ECC71']\n",
    "        markers = ['o', 's', 'D']\n",
    "        \n",
    "        for alg in labels:\n",
    "            phi_all = np.zeros((2, n_steps+1))\n",
    "            phi_all[:,0:1] = np.array([[-1.5],[6.5]])\n",
    "            momentum = np.zeros([2,1])\n",
    "            \n",
    "            for c_step in range(n_steps):\n",
    "                # Sample batch\n",
    "                batch_indices = np.random.choice(x.shape[0], batch_size, replace=False)\n",
    "                x_batch = x[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "                \n",
    "                # Compute gradient with L2\n",
    "                grad = compute_gradient_l2(x_batch, y_batch, phi_all[:,c_step:c_step+1], lmb)\n",
    "                \n",
    "                if alg == 'SGD':\n",
    "                    phi_all[:,c_step+1:c_step+2] = phi_all[:,c_step:c_step+1] - alpha * grad\n",
    "                elif alg == 'Momentum':\n",
    "                    momentum = beta * momentum + grad\n",
    "                    phi_all[:,c_step+1:c_step+2] = phi_all[:,c_step:c_step+1] - alpha * momentum\n",
    "                elif alg == 'Nesterov':\n",
    "                    # Lookahead\n",
    "                    phi_lookahead = phi_all[:,c_step:c_step+1] - alpha * beta * momentum\n",
    "                    grad_lookahead = compute_gradient_l2(x_batch, y_batch, phi_lookahead, lmb)\n",
    "                    momentum = beta * momentum + grad_lookahead\n",
    "                    phi_all[:,c_step+1:c_step+2] = phi_all[:,c_step:c_step+1] - alpha * momentum\n",
    "                    \n",
    "            alg_results.append(phi_all)\n",
    "\n",
    "        # Plotting for this lambda\n",
    "        offset_mesh, freq_mesh = np.meshgrid(np.arange(x_min, x_max, 0.2), np.arange(y_min, y_max, 0.2))\n",
    "        loss_mesh = np.zeros_like(offset_mesh)\n",
    "        for r in range(offset_mesh.shape[0]):\n",
    "            for c in range(offset_mesh.shape[1]):\n",
    "                phi_mesh = np.array([[offset_mesh[r,c]], [freq_mesh[r,c]]])\n",
    "                loss_mesh[r,c] = compute_loss_l2(x, y, model, phi_mesh, lmb)\n",
    "\n",
    "        # Draw contours\n",
    "        levels = np.geomspace(np.min(loss_mesh)+0.1, np.max(loss_mesh), 20)\n",
    "        ax.contour(offset_mesh, freq_mesh, loss_mesh, levels=levels, alpha=0.4, cmap='viridis')\n",
    "        \n",
    "        # Draw paths\n",
    "        for j, res in enumerate(alg_results):\n",
    "            ax.plot(res[0,:], res[1,:], color=colors[j], linewidth=3.5, label=labels[j], alpha=0.9, zorder=3)\n",
    "            # More frequent markers (every 5 steps) and larger points\n",
    "            ax.scatter(res[0,::5], res[1,::5], color=colors[j], marker=markers[j], s=80, zorder=5)\n",
    "            # Larger final point (Star)\n",
    "            ax.scatter(res[0,-1], res[1,-1], color=colors[j], marker='*', s=400, edgecolors='black', linewidth=1.5, zorder=10)\n",
    "\n",
    "        # Stronger start marker\n",
    "        ax.scatter(-1.5, 6.5, color='black', marker='X', s=200, label='Start', zorder=20)\n",
    "        \n",
    "        title = 'No Regularization (Lambda = 0)' if lmb == 0 else f'L2 Regularization (Lambda = {lmb:.2f})'\n",
    "        ax.set_title(title, fontsize=22, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Offset', fontsize=18)\n",
    "        ax.set_ylabel('Frequency', fontsize=18)\n",
    "        ax.set_xlim([x_min, x_max])\n",
    "        ax.set_ylim([y_min, y_max])\n",
    "        ax.grid(True, linestyle='--', alpha=0.4)\n",
    "        ax.tick_params(labelsize=16)\n",
    "        if idx == 1:\n",
    "            ax.legend(loc='upper right', fontsize=16, framealpha=0.9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive slider\n",
    "interact(run_and_plot, lmbda=FloatSlider(min=0.0, max=5.0, step=0.1, value=1.0, description='Lambda:'))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
